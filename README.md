# Natural Language Processing

## Embeddings

### a) Sentence
* 2018, Logeswaran et al.: An Efficient Framework for Learning Sentence Representations, ICLR, [[arXiv 2018]](https://arxiv.org/abs/1803.02893) [[OpenReview 2018]](https://openreview.net/forum?id=rJvJXZb0W) [[Notes]](https://github.com/ducthanhtran/paper_notes/blob/master/machine_learning/nlp/18_an_efficient_framework_for_learning_sent_repr.md)

### b) Documents
* 2014, Le et al.: Distributed Representations of Sentences and Documents, PMLR, [[PMLR]](http://proceedings.mlr.press/v32/le14.pdf) [[arXiv 2014]](https://arxiv.org/abs/1405.4053)

## Language Model
* 2017, Vania et al.: From Characters to Words to in Between: Do We Capture Morphology?, ACL, [[ACL 2017]](http://www.aclweb.org/anthology/P17-1184) [[arXiv 2017]](https://arxiv.org/abs/1704.08352)  [[Notes]](https://github.com/ducthanhtran/paper_notes/blob/master/machine_learning/nlp/17_from_characters_to_words_to_in_between_do_we_capture_morphology.md)
* 2017, Press et al.: Using the Output Embedding to Improve Language Models, ACL, [[arXiv 2016]](https://arxiv.org/abs/1608.05859) [[ACL 2017]](http://aclweb.org/anthology/E17-2025) [[Notes]](https://github.com/ducthanhtran/paper_notes/blob/master/machine_learning/nlp/17_using_the_output_embedding_to_improve_language_models.md)
* 2016, Kim et al.: Character-aware Neural Language Models, AAAI, [[arXiv 2015]](https://arxiv.org/abs/1508.06615) [[AAAI 2016]](https://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/viewFile/12489/12017)
* 2013, Mikolov et al.: Linguistic Regularities in Continuous Space Word Representation, NAACL-HLT, [[NAACL-HLT 2013]](https://www.aclweb.org/anthology/N13-1090) [[Notes]](https://github.com/ducthanhtran/paper_notes/blob/master/machine_learning/nlp/13_linguistic_regularities_in_continuous_space_word_representations.md)
* 2003, Bengio et al.: A neural probabilistic neural language model, JCML, [[JCML]](http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf) [[Notes]](https://github.com/ducthanhtran/paper_notes/blob/master/machine_learning/nlp/03_a_neural_probabilistic_language_model.md)


## Machine Translation
* 2017, Menacer et al.: Is Statistical Machine Translation Approach dead?, ICNLSSP, [[HAL 2017]](https://hal.inria.fr/hal-01660016/document) [[Notes]](https://github.com/ducthanhtran/paper_notes/blob/master/machine_learning/nlp/machine_translation/17_is_statistical_machine_translation_approach_dead.md)
* 2005, Ney: One Decade of Statistical Machine Translation: 1996-2005, CLIN, [[RWTH Aachen]](https://www-i6.informatik.rwth-aachen.de/publications/download/508/Ney-MT%20Summit-2005.pdf) [[Notes]](https://github.com/ducthanhtran/paper_notes/blob/master/machine_learning/nlp/machine_translation/05_one_decade_of_smt_1996_2005.md)

### Evaluation
* 2018 Dec, Domingo et al.: How Much Does Tokenization Affect Neural Machine Translation?, [[arXiv 2018]](https://arxiv.org/abs/1812.08621)
* 2018 Oct, Post: A Call for Clarity in Reporting BLEU Scores, WMT, [[WMT 2018]](http://aclweb.org/anthology/W18-6319) [[arXiv 2018]](https://arxiv.org/abs/1804.08771)
* 2018 Oct, LÃ¤ubli et al.: Has Machine Translation Achieved Human Parity? A Case for Document-level Evaluation, [[EMNLP 2018]](http://aclweb.org/anthology/D18-1512) [[arXiv 2018]](https://arxiv.org/pdf/1808.07048.pdf)
* 2002 Jul, Papineni et al.: BLEU: A Method for Automatic Evaluation of Machine Translation, ACL, [[ACL 2002]](https://www.aclweb.org/anthology/P02-1040.pdf)

### Search/Decoding
* 2018, Yang et al.: Breaking the Beam Search Curse: A Study of (Re-)Scoring Methods and Stopping Criteria for Neural Machine Translation, EMNLP, [[arXiv 2018]](https://arxiv.org/abs/1808.09582) [[EMNLP 2018]](http://aclweb.org/anthology/D18-1342)
* 2017, Freitag et al.: Beam Search Strategies for Neural Machine Translation, WNMT, [[WNMT 2017]](http://www.aclweb.org/anthology/W17-3207) [[arXiv 2017]](https://arxiv.org/abs/1702.01806)

### Modelling
* 2017, Britz et al.: Massive Exploration of Neural Machine Translation Architectures, ACL, [[arXiv 2017]](https://arxiv.org/abs/1703.03906) [[ACL 2017]](http://aclweb.org/anthology/D17-1151) [[Notes]](https://github.com/ducthanhtran/paper_notes/blob/master/machine_learning/nlp/machine_translation/17_massive_exploration_of_nmt_architectures.md)

#### a) Transformer-based
* 2018, Medina et al.: Parallel Attention Mechanism in Neural Machine Translation, ICMLA, [[arXiv 2018]](https://arxiv.org/abs/1810.12427)
* 2018, Shaw et al.: Self-Attention with Relative Position Representations, NAACL [[arXiv 2018]](https://arxiv.org/abs/1803.02155) [[Notes]](https://github.com/ducthanhtran/paper_notes/blob/master/machine_learning/nlp/machine_translation/18_self_attention_with_relative_position_representations.md)
* 2017, Vaswani et al.: Attention Is All You Need, NIPS, [[arXiv 2017]](https://arxiv.org/abs/1706.03762) [[NIPS 2017]](https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf) [[Kaiser: Slides]](https://nlp.stanford.edu/seminar/details/lkaiser.pdf) [[Notes]](https://github.com/ducthanhtran/paper_notes/blob/master/machine_learning/nlp/machine_translation/17_attention_is_all_you_need.md)

#### b) RNN
* 2017, Barone et al.: Deep Architectures for Neural Machine Translation, WMT, [[arXiv 2017]](https://arxiv.org/abs/1707.07631) [[WMT 2017]](http://www.aclweb.org/anthology/W17-4710) [[Notes]](https://github.com/ducthanhtran/paper_notes/blob/master/machine_learning/nlp/machine_translation/17_deep_architectures_for_nmt.md)

#### c) CNN
* 2017, Gehring et al.: A Convolutional Encoder Model for Neural Machine Translation, ACL, [[arXiv 2016]](https://arxiv.org/abs/1611.02344) [[ACL 2017]](http://www.aclweb.org/anthology/P17-1012) [[Notes]](https://github.com/ducthanhtran/paper_notes/blob/master/machine_learning/nlp/machine_translation/17_a_convolutional_encoder_mode_for_nmt.md)

### Attention Mechanism
* 2015, Bahdanau et al.: Neural Machine Translation by Jointly Learning to Align and Translate, ICLR, [[arXiv 2014]](https://arxiv.org/pdf/1409.0473.pdf) [[Slides, ICLR]](https://iclr.cc/archive/www/lib/exe/fetch.php%3Fmedia=iclr2015:bahdanau-iclr2015.pdf)

### Document-level Context
#### i) Local
* 2018 Oct, Zhang et al.: Improving the Transformer Translation Model with Document-level Context, EMNLP, [[EMNLP 2018]](http://aclweb.org/anthology/D18-1049)
* 2018 Oct, Cao et al.: Encoding Gated Translation Memory into Neural Machine Translation, EMNLP, [[EMNLP 2018]](http://aclweb.org/anthology/D18-1340)
* 2018 Aug, Kuang et al.: Fusing Recency into Neural Machine Translation with an Inter-Sentence Gate Model, [[COLING 2018]](http://aclweb.org/anthology/C18-1051)
* 2018 Jul, Voita et al.: Context-Aware Neural Machine Translation Learns Anaphora Resolution, ACL, [[arXiv 2018]](https://arxiv.org/abs/1805.10163) [[ACL 2018]](http://aclweb.org/anthology/P18-1117) [[Notes]](https://github.com/ducthanhtran/paper_notes/blob/master/machine_learning/nlp/machine_translation/18_context_aware_nmt_learns_anaphora_resolution)
* 2017 Apr, Jean et al.: Does Neural Machine Translation Benefit from Larger Context?, arXiv, [[arXiv 2017]](https://arxiv.org/pdf/1704.05135) [[Notes]](https://github.com/ducthanhtran/paper_notes/blob/master/machine_learning/nlp/machine_translation/17_does_nmt_benefit_from_larger_context.md)

#### ii) Global
* 2017, Wang et al.: Exploiting Cross-Sentence Context for Neural Machine Translation, ACL, [[arXiv 2017]](https://arxiv.org/abs/1704.04347.pdf) [[ACL 2017]](http://aclweb.org/anthology/D17-1301) [[Notes]](https://github.com/ducthanhtran/paper_notes/blob/master/machine_learning/nlp/machine_translation/17_exploiting_cross_sentence_context_for_nmt.md)

### Character-level
* 2017, Lee et al.: Fully Character-level Neural Machine Translation without Explicit Segmentation, TACL, [[arXiv 2016]](https://arxiv.org/abs/1610.03017) [[TACL 2017]](https://transacl.org/ojs/index.php/tacl/article/viewFile/1051/253) [[notes]](https://github.com/ducthanhtran/paper_notes/blob/master/machine_learning/nlp/machine_translation/16_fully_character_level_nmt_without_explicit_segmentation.md)
* 2016, Chung et al.: A Character-level Decoder without Explicit Segmentation for Neural Machine Translation, ACL, [[arXiv 2016]](https://arxiv.org/abs/1603.06147) [[ACL 2016]](http://www.aclweb.org/anthology/P16-1160) [[Notes]](https://github.com/ducthanhtran/paper_notes/blob/master/machine_learning/nlp/machine_translation/16_a_character_level_decoder_without_explicit_segmentation_for_nmt.md)
* 2015, Ling et al.: Character-based Neural Machine Translation, arXiv, [[arXiv 2015]](https://arxiv.org/abs/1511.04586.pdf) [[Notes]](https://github.com/ducthanhtran/paper_notes/blob/master/machine_learning/nlp/machine_translation/15_character_based_nmt.md)

### Use additional Monolingual Data
* 2018, Zhang et al.: Joint Training for Neural Machine Translation Models with Monolingual Data, AAAI, [[arXiv 2018]](https://arxiv.org/abs/1803.00353) [[Notes]](https://github.com/ducthanhtran/paper_notes/blob/master/machine_learning/nlp/machine_translation/18_joint_training_for_nmt_models_with_monolingual_data.md)
* 2016, Sennrich et al.: Improving Neural Machine Translation with Monolingual Data, ACL, [[arXiv 2015]](https://arxiv.org/abs/1511.06709.pdf) [[ACL 2016]](http://www.aclweb.org/anthology/P16-1009) [[Notes]](https://github.com/ducthanhtran/paper_notes/blob/master/machine_learning/nlp/machine_translation/15_improving_nmt_with_monolingual_data.md)
* 2016, Zhang et al.: Exploiting Source-side Monolingual Data in Neural Machine Translation, EMNLP, [[EMNLP 2016]](http://www.aclweb.org/anthology/D16-1160) [[Notes]](https://github.com/ducthanhtran/paper_notes/blob/master/machine_learning/nlp/machine_translation/16_exploiting_source_side_monolingual_data_in_nmt.md)

### Transfer Learning
* 2018, Tiedemann: Emerging Language Spaces Learned From Massively Multilingual Corpora, DHN, [[arXiv 2018]](https://arxiv.org/abs/1802.00273) [[Notes]](https://github.com/ducthanhtran/paper_notes/blob/master/machine_learning/nlp/machine_translation/18_emerging_language_spaces_learned_from_massively_multilingual_corpora.md)

#### Domain Adaption
* 2018 Oct, Thompson et al.: Freezing Subnetworks to Analyze Domain Adaption in Neural Machine Translation, WMT, [[WMT 2018]](http://aclweb.org/anthology/W18-6313.pdf) [[arXiv 2018]](https://arxiv.org/abs/1809.05218)
* 2017 Sep, Barone et al.: Regularization techniques for fine-tuning in neural machine translation, EMNLP, [[arXiv 2017]](https://arxiv.org/abs/1707.09920) [[EMNLP 2017]](https://www.aclweb.org/anthology/D17-1156)

### Low-resource Setting
* 2018, Gu et al.: Universal Neural Machine Translation for Extremely Low Resource Languages, AAAI, [[arXiv 2018]](https://arxiv.org/abs/1802.05368) [[Microsoft 2018]](https://www.microsoft.com/en-us/research/publication/universal-neural-machine-translation-extremely-low-resource-languages/) [[Notes]](https://github.com/ducthanhtran/paper_notes/blob/master/machine_learning/nlp/machine_translation/18_universal_nmt_for_extremely_low_resource_lang.md)

### Rare Word Problem
* 2016, Sennrich et al.: Neural Machine Translation of Rare Words with Subword Units, ACL, [[arXiv]](https://arxiv.org/abs/1508.07909) [[ACL 2016]](http://www.aclweb.org/anthology/P16-1162) [[Notes]](https://github.com/ducthanhtran/paper_notes/blob/master/machine_learning/nlp/machine_translation/16_nmt_of_rare_words_with_subword_units.md)
* 2015, Luong et al.: Addressing the Rare Word Problem in Neural Machine Translation, ACL, [[arXiv]](https://arxiv.org/abs/1410.8206) [[ACL 2015]](http://www.aclweb.org/anthology/P15-1002) [[Notes]](https://github.com/ducthanhtran/paper_notes/blob/master/machine_learning/nlp/machine_translation/15_addressing_the_rare_word_problem_in_neural_machine_translation.md)

# Neural Network Model
* 2015, Jozefowicz et al.: An Empirical Exploration of Recurrent Network Architectures, ICML, [[PMLR 2015]](http://proceedings.mlr.press/v37/jozefowicz15.pdf) [[Notes]](https://github.com/ducthanhtran/paper_notes/blob/master/machine_learning/neural_networks/15_an_empirical_exploration_of_recurrent_network_architectures.md)
* 1988, Levin et al.: Accelerated learning in layered neural networks, Complex Systems 2, [[Wolfram]](http://wpmedia.wolfram.com/uploads/sites/13/2018/02/02-6-1.pdf) [[Notes]](https://github.com/ducthanhtran/paper_notes/blob/master/machine_learning/neural_networks/88_accelerated_learning_in_layered_neural_networks.md)


## Architectures
* 2015, Srivastava et al.: Training Very Deep Networks, NIPS, [[arXiv 2015]](https://arxiv.org/abs/1507.06228) [[NIPS 2015]](https://papers.nips.cc/paper/5850-training-very-deep-networks.pdf) [[Preliminary paper - arXiv 2015]](https://arxiv.org/abs/1505.00387)

## Training
* 2015, Ioffe et al.: Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift, PMLR, [[PMLR 2015]](http://proceedings.mlr.press/v37/ioffe15.pdf) [[arXiv 2015]](https://arxiv.org/abs/1502.03167)

### Sequence-to-Sequence
* 2018, Chen et al.: Stable and Effective Trainable Greedy Decoding for Sequence to Sequence Learning, ICLR, [[OpenReview]](https://openreview.net/forum?id=rJZlKFkvM) [[Notes]](https://github.com/ducthanhtran/paper_notes/blob/master/machine_learning/nlp/machine_translation/18_stable_and_effective_trainable_greedy_decoding_for_seq_to_seq_learning.md)
* 2014, Sutskever et al.: Sequence to Sequence Learning with Neural Networks, NIPS, [[arXiv 2014]](https://arxiv.org/abs/1409.3215) [[NIPS 2014]](https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf)

# Tools
* 2018, Peter et al.: Sisyphus, a Workflow Manager Designed for Machine Translatin and Automatic Speech Recognition, EMNLP, [[EMNLP 2018]](http://aclweb.org/anthology/D18-2015)
