# Natural Language Processing

## Sentence Embeddings

* 2018, Logeswaran et al.: An Efficient Framework for Learning Sentence Representations, ICLR, [[arXiv 2018]](https://arxiv.org/abs/1803.02893) [[OpenReview 2018]](https://openreview.net/forum?id=rJvJXZb0W) [[Notes]](https://github.com/ducthanhtran/paper_notes/blob/master/machine_learning/nlp/18_an_efficient_framework_for_learning_sent_repr.md)

## Language Model
* 2017, Vania et al.: From Characters to Words to in Between: Do We Capture Morphology?, ACL, [[arXiv 2017]](https://arxiv.org/abs/1704.08352) [[ACL 2017]](http://www.aclweb.org/anthology/P17-1184) [[Notes]](https://github.com/ducthanhtran/paper_notes/blob/master/machine_learning/nlp/17_from_characters_to_words_to_in_between_do_we_capture_morphology.md)
* 2017, Press et al.: Using the Output Embedding to Improve Language Models, ACL, [[arXiv 2016]](https://arxiv.org/abs/1608.05859) [[ACL 2017]](http://aclweb.org/anthology/E17-2025) [[Notes]](https://github.com/ducthanhtran/paper_notes/blob/master/machine_learning/nlp/17_using_the_output_embedding_to_improve_language_models.md)
* 2013, Mikolov et al.: Linguistic Regularities in Continuous Space Word Representation, NAACL-HLT, [[NAACL-HLT 2013]](https://www.aclweb.org/anthology/N13-1090) [[Notes]](https://github.com/ducthanhtran/paper_notes/blob/master/machine_learning/nlp/13_linguistic_regularities_in_continuous_space_word_representations.md)
* 2003, Bengio et al.: A neural probabilistic neural language model, JCML, [[JCML]](http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf) [[Notes]](https://github.com/ducthanhtran/paper_notes/blob/master/machine_learning/nlp/03_a_neural_probabilistic_language_model.md)


## Machine Translation
* 2017, Menacer et al.: Is Statistical Machine Translation Approach dead?, ICNLSSP, [[HAL 2017]](https://hal.inria.fr/hal-01660016/document) [[Notes]](https://github.com/ducthanhtran/paper_notes/blob/master/machine_learning/nlp/machine_translation/17_is_statistical_machine_translation_approach_dead.md)
* 2005, Ney: One Decade of Statistical Machine Translation: 1996-2005, CLIN, [[RWTH Aachen]](https://www-i6.informatik.rwth-aachen.de/publications/download/508/Ney-MT%20Summit-2005.pdf) [[Notes]](https://github.com/ducthanhtran/paper_notes/blob/master/machine_learning/nlp/machine_translation/05_one_decade_of_smt_1996_2005.md)

### Evaluation
* 2018, Domingo et al.: How Much Does Tokenization Affect Neural Machine Translation?, [[arXiv 2018]](https://arxiv.org/abs/1812.08621)
* 2002, Papineni et al.: BLEU: A Method for Automatic Evaluation of Machine Translation, ACL, [[ACL 2002]](https://www.aclweb.org/anthology/P02-1040.pdf)

### Modelling
* 2017, Britz et al.: Massive Exploration of Neural Machine Translation Architectures, ACL, [[arXiv 2017]](https://arxiv.org/abs/1703.03906) [[ACL 2017]](http://aclweb.org/anthology/D17-1151) [[Notes]](https://github.com/ducthanhtran/paper_notes/blob/master/machine_learning/nlp/machine_translation/17_massive_exploration_of_nmt_architectures.md)

#### a) Transformer-based
* 2018, Shaw et al.: Self-Attention with Relative Position Representations, NAACL [[arXiv 2018]](https://arxiv.org/abs/1803.02155) [[Notes]](https://github.com/ducthanhtran/paper_notes/blob/master/machine_learning/nlp/machine_translation/18_self_attention_with_relative_position_representations.md)
* 2017, Vaswani et al.: Attention Is All You Need, NIPS, [[arXiv 2017]](https://arxiv.org/abs/1706.03762) [[NIPS 2017]](https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf) [[Kaiser: Slides]](https://nlp.stanford.edu/seminar/details/lkaiser.pdf) [[Notes]](https://github.com/ducthanhtran/paper_notes/blob/master/machine_learning/nlp/machine_translation/17_attention_is_all_you_need.md)

#### b) RNN
* 2017, Barone et al.: Deep Architectures for Neural Machine Translation, WMT, [[arXiv 2017]](https://arxiv.org/abs/1707.07631) [[WMT 2017]](http://www.aclweb.org/anthology/W17-4710) [[Notes]](https://github.com/ducthanhtran/paper_notes/blob/master/machine_learning/nlp/machine_translation/17_deep_architectures_for_nmt.md)

#### c) CNN
* 2017, Gehring et al.: A Convolutional Encoder Model for Neural Machine Translation, ACL, [[arXiv 2016]](https://arxiv.org/abs/1611.02344) [[ACL 2017]](http://www.aclweb.org/anthology/P17-1012) [[Notes]](https://github.com/ducthanhtran/paper_notes/blob/master/machine_learning/nlp/machine_translation/17_a_convolutional_encoder_mode_for_nmt.md)

### Document-level Context
#### i) Local
* 2018, Voita et al.: Context-Aware Neural Machine Translation Learns Anaphora Resolution, ACL, [[arXiv 2018]](https://arxiv.org/abs/1805.10163) [[ACL 2018]](http://aclweb.org/anthology/P18-1117) [[Notes]](https://github.com/ducthanhtran/paper_notes/blob/master/machine_learning/nlp/machine_translation/18_context_aware_nmt_learns_anaphora_resolution)
* 2017, Jean et al.: Does Neural Machine Translation Benefit from Larger Context?, arXiv, [[arXiv 2017]](https://arxiv.org/pdf/1704.05135) [[Notes]](https://github.com/ducthanhtran/paper_notes/blob/master/machine_learning/nlp/machine_translation/17_does_nmt_benefit_from_larger_context.md)

#### ii) Global
* 2017, Wang et al.: Exploiting Cross-Sentence Context for Neural Machine Translation, ACL, [[arXiv 2017]](https://arxiv.org/abs/1704.04347.pdf) [[ACL 2017]](http://aclweb.org/anthology/D17-1301) [[Notes]](https://github.com/ducthanhtran/paper_notes/blob/master/machine_learning/nlp/machine_translation/17_exploiting_cross_sentence_context_for_nmt.md)

### Character-level
* 2017, Lee et al.: Fully Character-level Neural Machine Translation without Explicit Segmentation, TACL, [[arXiv 2016]](https://arxiv.org/abs/1610.03017) [[TACL 2017]](https://transacl.org/ojs/index.php/tacl/article/viewFile/1051/253) [[notes]](https://github.com/ducthanhtran/paper_notes/blob/master/machine_learning/nlp/machine_translation/16_fully_character_level_nmt_without_explicit_segmentation.md)
* 2016, Chung et al.: A Character-level Decoder without Explicit Segmentation for Neural Machine Translation, ACL, [[arXiv 2016]](https://arxiv.org/abs/1603.06147) [[ACL 2016]](http://www.aclweb.org/anthology/P16-1160) [[Notes]](https://github.com/ducthanhtran/paper_notes/blob/master/machine_learning/nlp/machine_translation/16_a_character_level_decoder_without_explicit_segmentation_for_nmt.md)
* 2015, Ling et al.: Character-based Neural Machine Translation, arXiv, [[arXiv 2015]](https://arxiv.org/abs/1511.04586.pdf) [[Notes]](https://github.com/ducthanhtran/paper_notes/blob/master/machine_learning/nlp/machine_translation/15_character_based_nmt.md)

### Use additional Monolingual Data
* 2018, Zhang et al.: Joint Training for Neural Machine Translation Models with Monolingual Data, AAAI, [[arXiv 2018]](https://arxiv.org/abs/1803.00353) [[Notes]](https://github.com/ducthanhtran/paper_notes/blob/master/machine_learning/nlp/machine_translation/18_joint_training_for_nmt_models_with_monolingual_data.md)
* 2016, Sennrich et al.: Improving Neural Machine Translation with Monolingual Data, ACL, [[arXiv 2015]](https://arxiv.org/abs/1511.06709.pdf) [[ACL 2016]](http://www.aclweb.org/anthology/P16-1009) [[Notes]](https://github.com/ducthanhtran/paper_notes/blob/master/machine_learning/nlp/machine_translation/15_improving_nmt_with_monolingual_data.md)
* 2016, Zhang et al.: Exploiting Source-side Monolingual Data in Neural Machine Translation, EMNLP, [[EMNLP 2016]](http://www.aclweb.org/anthology/D16-1160) [[Notes]](https://github.com/ducthanhtran/paper_notes/blob/master/machine_learning/nlp/machine_translation/16_exploiting_source_side_monolingual_data_in_nmt.md)

### Transfer Learning
* 2018, Tiedemann: Emerging Language Spaces Learned From Massively Multilingual Corpora, DHN, [[arXiv 2018]](https://arxiv.org/abs/1802.00273) [[Notes]](https://github.com/ducthanhtran/paper_notes/blob/master/machine_learning/nlp/machine_translation/18_emerging_language_spaces_learned_from_massively_multilingual_corpora.md)

### Low-resource Setting
* 2018, Gu et al.: Universal Neural Machine Translation for Extremely Low Resource Languages, AAAI, [[arXiv 2018]](https://arxiv.org/abs/1802.05368) [[Microsoft 2018]](https://www.microsoft.com/en-us/research/publication/universal-neural-machine-translation-extremely-low-resource-languages/) [[Notes]](https://github.com/ducthanhtran/paper_notes/blob/master/machine_learning/nlp/machine_translation/18_universal_nmt_for_extremely_low_resource_lang.md)

### Rare Word Problem
* 2016, Sennrich et al.: Neural Machine Translation of Rare Words with Subword Units, ACL, [[arXiv]](https://arxiv.org/abs/1508.07909) [[ACL 2016]](http://www.aclweb.org/anthology/P16-1162) [[Notes]](https://github.com/ducthanhtran/paper_notes/blob/master/machine_learning/nlp/machine_translation/16_nmt_of_rare_words_with_subword_units.md)
* 2015, Luong et al.: Addressing the Rare Word Problem in Neural Machine Translation, ACL, [[arXiv]](https://arxiv.org/abs/1410.8206) [[ACL 2015]](http://www.aclweb.org/anthology/P15-1002) [[Notes]](https://github.com/ducthanhtran/paper_notes/blob/master/machine_learning/nlp/machine_translation/15_addressing_the_rare_word_problem_in_neural_machine_translation.md)

# Neural Network Model
* 2015, Jozefowicz et al.: An Empirical Exploration of Recurrent Network Architectures, ICML, [[PMLR 2015]](http://proceedings.mlr.press/v37/jozefowicz15.pdf) [[Notes]](https://github.com/ducthanhtran/paper_notes/blob/master/machine_learning/neural_networks/15_an_empirical_exploration_of_recurrent_network_architectures.md)
* 1988, Levin et al.: Accelerated learning in layered neural networks, Complex Systems 2, [[Wolfram]](http://wpmedia.wolfram.com/uploads/sites/13/2018/02/02-6-1.pdf) [[Notes]](https://github.com/ducthanhtran/paper_notes/blob/master/machine_learning/neural_networks/88_accelerated_learning_in_layered_neural_networks.md)

## Sequence-to-Sequence
* 2018, Chen et al.: Stable and Effective Trainable Greedy Decoding for Sequence to Sequence Learning, ICLR, [[OpenReview]](https://openreview.net/forum?id=rJZlKFkvM) [[Notes]](https://github.com/ducthanhtran/paper_notes/blob/master/machine_learning/nlp/machine_translation/18_stable_and_effective_trainable_greedy_decoding_for_seq_to_seq_learning.md)

