# Exploiting Cross-sentence Context for Neural Machine Translation
* ArXiv April 2017
* EMNLP September 2017

**Authors**:
* Wang, Longyue
* Tu, Zhaopeng
* Way, Andy
* Liu, Qun

## Key points
* Take context of source text into consideration during translation: less ambiguity and inconsistencies.
* Hierarchical context structure: from sentences to document level. Consider last K sentences as context and summarize these with *sentence RNNs*. Now, the *document RNN* takes as input the last hidden states of the *sentence RNNs* and compute a global (cross-sentence) context state D.
* Strategies of integrating global context:
    1. Use D for initialization of encoder/decoder
        * Encoder: D as initialization state instead of zero-state in RNNs
        * Decoder: add to last hidden state of Encoder and obtain a modified initial state
    2. Auxiliary Context
        * D as additional input when computing hidden decoder state s_i.
        * concatenation with attention-based context vector c_i
    3. Gating Auxiliary Context
        * Extension of ii.: dynamically control influence of D by using **context gating** mechanism

## Experiments
* Chinese-English, 1M sentence pairs from LDC corpora
    * 25.4M chin. words and 32.4M engl. words
    * Development set: NIST05
    * Test sets: NIST06, NIST08
    * Baselines: NEMATUS (NMT) and MOSES (SMT)
    * set K = 3 for all models

### Results
1. Initialization strategies: around +1.0 to +1.3 BLEU points
    * when initializing both encoder and decoder: +1.4 BLEU points
2. Auxiliary Context: +0.7 BLEU points
3. Gating Auxiliary Context: +1.67 BLEU points
4. Initialize both encoder/decoder and use gating aux. context: +2.1 BLEU points

* decreases ambiguity and inconsistency errors

## Notes
* gating mechanism is more flexible than just concatenating hidden states
### Drawbacks
- no investigation of K parameter
- no parallelization possible due to context computation
### Benefits
+ easy way to incorporate context
