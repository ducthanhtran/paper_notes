# Exploiting Source-side monolingual Data in Neural Machine Translation
* EMNLP November 2016

**Authors**:
* Zhang, Jiajun
* Zong, Chengqing

## Key points
* use monolingual data in order to improve translation systems as bilingual data are scarce (limited domains, low-ressource language pairs)
* focus on encoder module (source-side)
* two methods for incorporating source-side monolingual corpus in NMT
    1. Self-learning method:
        * like [Backtranslation](https://github.com/ducthanhtran/paper_notes/blob/master/neural_machine_translation/15nov_improving_nmt_with_monolingual_data.md) from Sennrich et al., 2015: translate monolingual data with baseline machine translation system in order to obtain a dummy parallel corpus (synthetic data)
        * train final NMT model with original plus synthetic corpus
        * freeze decoder's parameters while training on synthetic data
    2. Sentence re-ordering method: translation and source-side sentence re-ordering tasks
        * multi-task learning
        * one shared encoder and two decoders for each task
            * re-ordering task: permutate words of source sentence; view as special machine translation task
        * learning process: 1 epoch re-ordering, then 4 epochs translation - always transfer encoder parameters


## Experiments
* Chinese-English from LDC corpora
    * small dataset: 0.63M sentence pairs
    * large dataset: 2.1M sentence pairs + small dataset
    * monolingual data: extract 20M Chinese sentences from LDC and filter: >= 50% words appear in source-side bilingual training data
        * 6.5M sentences for small-scale experiment
        * 12M sentences for large-scale experiment
    * development set: NIST03
    * test sets: NIST04, NIST05, NIST06
    * baselines: MOSES (SMT) and RNNSearch (Bahdanau et al., 2014)

### Results
* with different scales of monolingual data (according to word coverage)
* proposed methods more effective than using autoencoders in multi-task setup (Luong et al., 2015)
* comparison with RNNSearch that has about -1.4 BLEU scores than MOSES
* small-scale experiment
    1. Self-learning method: +1.2 BLEU to +1.6 BLEU (at top 50% monoling. data)
    2. Sentence re-ordering method: +1.7 BLEU to 2.1 BLEU (at top 50% monoling. data)
* large-scale experiment
    1. *not mentioned by authors*
    2. Sentence re-ordering method: +0.7 to +1.4 BLEU (at top 50% monoling. data)

## Notes
* adding out-of-domain monolingual data degrades translation quality
* improvement on large-scale billingual corpus is weaker
* Multi-task framework has higher complexity but better BLEU improvements
### Drawbacks
- investigation on large-scale experiment is fairly short
### Benefits
+ techniques can be used for any encoder-decoder architecture
